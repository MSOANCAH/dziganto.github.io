{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "---\n",
    "\n",
    "Most machine learning algorithms are batch learners meaning they generate a model by learning on the entire dataset at one time. Not surprisingly, these algorithms are the most well known and most used. However, there is another class of algorithms known as online learning algorithms. Intead of learning on the entire dataset at once, data is consumed in sequential order as it becomes available. Said another way, online learning is a way to dynamically update a model in real-time according to the most recent data. For a more detailed discussion, see [Online Machine Learning](https://en.wikipedia.org/wiki/Online_machine_learning).\n",
    "\n",
    "Both batch and online learning have advantages and disadvantages. This should come as no surprise to anyone who has dabbled in machine learning. Rarely if ever do you get something for nothing; there are always tradeoffs. For a discussion on the pros and cons, see [this](https://www.quora.com/What-are-the-pros-and-cons-of-offline-vs-online-learning) Quora post.\n",
    "\n",
    "## Implementation\n",
    "---\n",
    "Now that we've got that out of the way, let's walkthrough how to implement an online learner in Scikit-learn. First, let's introduce the datasets.\n",
    "\n",
    "1. [Iris dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris)  \n",
    "\n",
    "2. [Boston dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html#sklearn.datasets.load_boston)\n",
    "\n",
    "The Iris dataset is a famous, canonical classification dataset. See the docs for details. The Boston dataset is a canonical regression dataset. \n",
    "\n",
    "*Note: The Iris dataset has 3 classes. This is not ideal. Introducing online learning with a binary class problem would have been better but that requires sourcing a binary dataset and likely going through preprocessing. Therefore, I decided that although 3 classes, which will require a One-Versus-All (OVA) approach - is not ideal, it made sense to me to keep the data ingestion process as streamlined and as simple as possible. Hence, why I chose to load both datasets from Scikit-learn.*\n",
    "\n",
    "\n",
    "Without further ado, see the code below for an implementation of online learning in Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Version\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.3 :: Anaconda custom (x86_64)\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries & Versions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn version: 0.18.2\n"
     ]
    }
   ],
   "source": [
    "import cpuinfo\n",
    "import sklearn\n",
    "from sklearn.datasets import load_iris, load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
    "from sklearn.metrics import log_loss, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Output library versions\n",
    "items = [(\"Sklearn\", sklearn)]\n",
    "for item in items:\n",
    "    print(item[0] + \" version: \" + str(item[1].__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Specs\n",
    "---\n",
    "The hardware specifications of my 2015 Mac are included for comparison purposes. Performance is a function of these specs, which processes are running in the background, and software implementation details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arch': 'X86_64',\n",
       " 'bits': 64,\n",
       " 'brand': 'Intel(R) Core(TM) i7-5557U CPU @ 3.10GHz',\n",
       " 'l2_cache_size': '256',\n",
       " 'vendor_id': 'GenuineIntel'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if not installed, type: python -m pip install -U py-cpuinfo\n",
    "\n",
    "info = cpuinfo.get_cpu_info()\n",
    "entries = ('flags', 'count', 'cpuinfo_version', 'family', 'hz_actual', 'hz_actual_raw', 'hz_advertised', \n",
    "          'hz_advertised_raw', 'model', 'raw_arch_string', 'stepping')\n",
    "for key in entries:\n",
    "        if key in info:\n",
    "            del info[key]\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "---\n",
    "Here we load the Iris and Boston datasets. Keep in mind that Scikit-learn neatly packages these datasets in a convenient dictionary style making it trivial to parse data and target. See the docs for details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classification\n",
    "iris = load_iris()\n",
    "\n",
    "# Regression\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Split\n",
    "---\n",
    "Each dataset is split using a test size of 20%. A random seed is provided for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris dataset\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(iris.data, \n",
    "                                                                        iris.target, \n",
    "                                                                        test_size=0.2, \n",
    "                                                                        random_state=42)\n",
    "\n",
    "# boston dataset\n",
    "X_train_boston, X_test_boston, y_train_boston, y_test_boston = train_test_split(boston.data,\n",
    "                                                                                boston.target,\n",
    "                                                                                test_size=0.2, \n",
    "                                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "svm = SGDClassifier(loss='hinge', \n",
    "                    penalty='l2', \n",
    "                    alpha=0.1, \n",
    "                    fit_intercept=False, \n",
    "                    n_iter=5, \n",
    "                    shuffle=True, \n",
    "                    verbose=1, \n",
    "                    n_jobs=1, \n",
    "                    random_state=19, \n",
    "                    learning_rate='optimal', \n",
    "                    class_weight='balanced')\n",
    "\n",
    "logistic = SGDClassifier(loss='log', \n",
    "                         penalty='l2', \n",
    "                         alpha=0.1, \n",
    "                         fit_intercept=False, \n",
    "                         n_iter=5, \n",
    "                         shuffle=True, \n",
    "                         verbose=1, \n",
    "                         n_jobs=1,\n",
    "                         random_state=19, \n",
    "                         learning_rate='optimal', \n",
    "                         class_weight='balanced')\n",
    "\n",
    "# Regression\n",
    "ols = SGDRegressor(loss='squared_loss', \n",
    "                   penalty='l2', \n",
    "                   alpha=0.0001, \n",
    "                   fit_intercept=False, \n",
    "                   n_iter=5, \n",
    "                   shuffle=True, \n",
    "                   verbose=1, \n",
    "                   random_state=42, \n",
    "                   learning_rate='invscaling', \n",
    "                   eta0=0.01, \n",
    "                   power_t=0.5)\n",
    "\n",
    "robust = SGDRegressor(loss='huber', \n",
    "                   penalty='l2', \n",
    "                   alpha=0.0001, \n",
    "                   fit_intercept=False, \n",
    "                   n_iter=5, \n",
    "                   shuffle=True, \n",
    "                   verbose=1, \n",
    "                   epsilon=0.1, \n",
    "                   random_state=42, \n",
    "                   learning_rate='invscaling', \n",
    "                   eta0=0.01, \n",
    "                   power_t=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Data\n",
    "---\n",
    "We'll utilize gradient descent in our online learners so it's best to standardize our data with mean 0 and standard deviation of 1. Make sure to fit and transform on the training set only. Then use that fit to transform the test set. Unfortunately, many mistakenly standardize the data prior to splitting into train and test, which causes information leakage.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris\n",
    "sc_iris = StandardScaler()\n",
    "X_train_iris = sc_iris.fit_transform(X_train_iris)\n",
    "X_test_iris = sc_iris.transform(X_test_iris)\n",
    "\n",
    "# boston\n",
    "sc_boston = StandardScaler()\n",
    "X_train_boston = sc_boston.fit_transform(X_train_boston)\n",
    "X_test_boston = sc_boston.transform(X_test_boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train w/Online Learning Model: Classification\n",
    "---\n",
    "The Iris dataset has 3 classes so a One-Versus-All approach is taken in this online learning example. Therefore, there will be 3 runs, each with the specified number of epochs which I've set to 5.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1] Support Vector Machines: Output = Class ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 1.23, NNZs: 4, Bias: 0.000000, T: 120, Avg. loss: 0.077963\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.32, NNZs: 4, Bias: 0.000000, T: 240, Avg. loss: 0.083950\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.28, NNZs: 4, Bias: 0.000000, T: 360, Avg. loss: 0.083947\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.31, NNZs: 4, Bias: 0.000000, T: 480, Avg. loss: 0.083557\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.30, NNZs: 4, Bias: 0.000000, T: 600, Avg. loss: 0.083154\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 1.02, NNZs: 4, Bias: 0.000000, T: 120, Avg. loss: 0.899049\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.75, NNZs: 4, Bias: 0.000000, T: 240, Avg. loss: 0.822984\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.89, NNZs: 4, Bias: 0.000000, T: 360, Avg. loss: 0.797535\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.92, NNZs: 4, Bias: 0.000000, T: 480, Avg. loss: 0.781262\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.98, NNZs: 4, Bias: 0.000000, T: 600, Avg. loss: 0.769156\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 0.93, NNZs: 4, Bias: 0.000000, T: 120, Avg. loss: 0.549069\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.96, NNZs: 4, Bias: 0.000000, T: 240, Avg. loss: 0.499881\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.89, NNZs: 4, Bias: 0.000000, T: 360, Avg. loss: 0.478825\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.90, NNZs: 4, Bias: 0.000000, T: 480, Avg. loss: 0.467435\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.88, NNZs: 4, Bias: 0.000000, T: 600, Avg. loss: 0.459986\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=False, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=19, shuffle=True, verbose=1,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(X_train_iris, y_train_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the SVM performs from an accuracy perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90000000000000002"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(X_test_iris, y_test_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2] Logistic Regression: Output = Class Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 1.38, NNZs: 4, Bias: 0.000000, T: 120, Avg. loss: 0.181071\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.40, NNZs: 4, Bias: 0.000000, T: 240, Avg. loss: 0.180590\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.40, NNZs: 4, Bias: 0.000000, T: 360, Avg. loss: 0.179628\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.40, NNZs: 4, Bias: 0.000000, T: 480, Avg. loss: 0.179206\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.40, NNZs: 4, Bias: 0.000000, T: 600, Avg. loss: 0.178853\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 0.71, NNZs: 4, Bias: 0.000000, T: 120, Avg. loss: 0.649057\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.61, NNZs: 4, Bias: 0.000000, T: 240, Avg. loss: 0.627146\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.70, NNZs: 4, Bias: 0.000000, T: 360, Avg. loss: 0.619320\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.67, NNZs: 4, Bias: 0.000000, T: 480, Avg. loss: 0.615505\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.69, NNZs: 4, Bias: 0.000000, T: 600, Avg. loss: 0.612655\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 1.06, NNZs: 4, Bias: 0.000000, T: 120, Avg. loss: 0.443592\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.00, NNZs: 4, Bias: 0.000000, T: 240, Avg. loss: 0.433367\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.97, NNZs: 4, Bias: 0.000000, T: 360, Avg. loss: 0.428242\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.96, NNZs: 4, Bias: 0.000000, T: 480, Avg. loss: 0.425880\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.96, NNZs: 4, Bias: 0.000000, T: 600, Avg. loss: 0.424026\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.1, average=False, class_weight='balanced', epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=False, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=19, shuffle=True, verbose=1,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.fit(X_train_iris, y_train_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm:      0.9\n",
      "logistic: 0.9\n",
      "\n",
      "logistic log loss: 0.62\n",
      "svm cannot output log loss\n"
     ]
    }
   ],
   "source": [
    "class_models = (('svm:     ', svm), ('logistic:', logistic))\n",
    "for name, model in class_models:\n",
    "    print(name, model.score(X_test_iris, y_test_iris))\n",
    "    if model == logistic:\n",
    "        print(\"\\nlogistic log loss: %.2f\" % log_loss(y_test_iris, model.predict_proba(X_test_iris)))\n",
    "print(\"svm cannot output log loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log loss** is a better measure of performance than accuracy but only Logistic Regression outputs class probabilities which is required for log loss. \n",
    "\n",
    "Keep in mind that we only made 5 passes over the data. We surely could drive the log loss down with more passes, though we need to take care not to overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train w/Online Learning Model: Regression\n",
    "---\n",
    "We'll train Ordinary Least Squares (OLS) and a Robust learner that uses [Huber loss](https://en.wikipedia.org/wiki/Huber_loss) on the Boston dataset. Once fit, we'll assess each model by looking at the resulting Root Mean Squared Error (RMSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1] Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 3.29, NNZs: 13, Bias: 0.000000, T: 404, Avg. loss: 285.922195\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.74, NNZs: 13, Bias: 0.000000, T: 808, Avg. loss: 281.652235\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.98, NNZs: 13, Bias: 0.000000, T: 1212, Avg. loss: 279.705793\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.15, NNZs: 13, Bias: 0.000000, T: 1616, Avg. loss: 278.524070\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.32, NNZs: 13, Bias: 0.000000, T: 2020, Avg. loss: 277.698078\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=False, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', n_iter=5, penalty='l2', power_t=0.5,\n",
       "       random_state=42, shuffle=True, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols.fit(X_train_boston, y_train_boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2] Robust Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.01, NNZs: 13, Bias: 0.000000, T: 404, Avg. loss: 2.274738\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.01, NNZs: 13, Bias: 0.000000, T: 808, Avg. loss: 2.274710\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.01, NNZs: 13, Bias: 0.000000, T: 1212, Avg. loss: 2.274698\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.01, NNZs: 13, Bias: 0.000000, T: 1616, Avg. loss: 2.274691\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.01, NNZs: 13, Bias: 0.000000, T: 2020, Avg. loss: 2.274687\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=False, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='huber', n_iter=5, penalty='l2', power_t=0.5, random_state=42,\n",
       "       shuffle=True, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust.fit(X_train_boston, y_train_boston)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare RMSE\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ols:    23.5464882574\n",
      "robust: 23.1325721384\n"
     ]
    }
   ],
   "source": [
    "reg_models = (('ols:   ', ols), ('robust:', robust))\n",
    "for name, model in reg_models:\n",
    "    print(name, mean_squared_error(y_test_boston, model.predict(X_test_boston)) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the robust learner outperformed OLS because Huber loss is more robust to outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Thoughts\n",
    "---\n",
    "You likely have several questions at this point. Hopefully I can preemptively address some of them here. \n",
    "\n",
    "**Q:** *When should I use batch learning and when should I use online learning?*  \n",
    "**A:** Use batch learning unless: \n",
    "1. Your data does not fit into memory \n",
    "2. You expect your data to change significantly over time (though retraining over a sliding window can help batch)  \n",
    "\n",
    "**Q:** *How do I tune an online learning algorithm?*  \n",
    "**A:** The answer depends on your use case. For the sake of simplicity, let's assume you're using online learning because your data doesn't fit into memory. In that case, you can follow the standard practice of splitting your data and tuning your hyperparameters with cross-validation. If you expect your data to change significantly over time, splitting your data may not be feasible. In this case, you can leverage [progressive cross-validation](http://hunch.net/~jl/projects/prediction_bounds/progressive_validation/coltfinal.pdf) and run multiple models to ascertain which set of hyperparameters is likely to generalize best. If your model is in production, that's a whole different story. Then you really need to rely on someone with expertise in these types of algorithms. As an aside, it's often the case that a hybrid batch and online learning approach is used in production instead of solely relying on online learning.\n",
    "\n",
    "**Q:** *What are some good resources for delving deeper into online learning?*  \n",
    "**A:** Here are several to get you started:\n",
    "1. [CILVR Lab @ NYU](http://cilvr.cs.nyu.edu/doku.php?id=courses:bigdata:slides:start)  \n",
    "2. [Online Learning & Stochastic Approximations](http://leon.bottou.org/publications/pdf/online-1998.pdf)\n",
    "3. [Fractal Analytics Blog](http://blog.fractalanalytics.com/institutionalizing-analytics/online-machine-learning-2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "---\n",
    "Batch and online learning are two common approaches to machine learning. With batch, sometimes called offline learning, all data is consumed to build a model. In contrast, online learning algorithms consume single observations. Batch is more common and prevalent. However, there are many use cases for online learning. For example, online learning shines when data is too large to fit into memory or you expect your the distribution of your data to drift over time. Online learning is typically very fast and once data has been consumed, it's really not needed anymore. Those can of course be great benefits but like all things in machine learning, you have to make sure your approach makes sense for your use case. To use a cliche, think of online learning as another tool in your toolbox. Knowing when to use it is as important as having it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
